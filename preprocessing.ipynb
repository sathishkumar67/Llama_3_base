{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm  \n",
    "\n",
    "conversation = []\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file):\n",
    "  conversation = []\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "      chat = item[\"title\"]\n",
    "      for x, y in item[\"mapping\"].items():\n",
    "        if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "          try:\n",
    "            if len(y['message']['content']['parts'][0]) > 0:\n",
    "              chat += f\" {y['message']['author']['role'].capitalize()}: {y['message']['content']['parts'][0]}\"\n",
    "          except Exception as e:\n",
    "            pass\n",
    "      conversation.append(chat)\n",
    "    return conversation\n",
    "\n",
    "# Function to combine results from all files\n",
    "def process_all_files(files):\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        results = list(tqdm(executor.map(process_file, files), total=len(files)))\n",
    "\n",
    "    # Combine results into a single conversation list\n",
    "    all_conversations = []\n",
    "    for result in results:\n",
    "        all_conversations.extend(result)\n",
    "\n",
    "    return all_conversations\n",
    "\n",
    "# Get the list of all files\n",
    "files = [os.path.join(\"chatgpt_dataset\", file) for file in os.listdir(\"chatgpt_dataset\")]\n",
    "\n",
    "# Process the files and get the conversation data\n",
    "conversation = process_all_files(files)\n",
    "\n",
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"pt-sk/ll-3.2-1B_Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "# Use ThreadPoolExecutor to tokenize texts in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Map the tokenize_text function to each string in texts\n",
    "    tokenized_texts = list(executor.map(tokenize_text, conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902932"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "tokens = list(chain.from_iterable(tokenized_texts))\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the tokenized texts to a numpy file\n",
    "file = np.array(tokens)\n",
    "np.save(\"conversation_tokens.npy\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4178,    44,  5075, 31754, 40283,  5468,    51, 22312,  2724])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loaded = np.load(\"conversation_tokens.npy\", allow_pickle=True)\n",
    "file_loaded[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902932"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\sathi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conversation_tokens.npy: 100%|██████████| 7.61M/7.61M [00:04<00:00, 1.89MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/pt-sk/chatgpt-dataset/commit/057c30e74f49ca0714bdc8cba83de6a42b058c0b', commit_message='Upload conversation_tokens.npy with huggingface_hub', commit_description='', oid='057c30e74f49ca0714bdc8cba83de6a42b058c0b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_name = \"pt-sk/chatgpt-dataset\"\n",
    "\n",
    "# from huggingface_hub import HfApi, login\n",
    "# login(token)\n",
    "\n",
    "# api = HfApi()\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"conversation_tokens.npy\",\n",
    "#     path_in_repo=\"conversation_tokens.npy\",\n",
    "#     repo_id=file_name,\n",
    "#     repo_type=\"dataset\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conversation_tokens.npy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"pt-sk/chatgpt-dataset\", filename=\"conversation_tokens.npy\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4178,    44,  5075, 31754, 40283,  5468,    51, 22312,  2724]),\n",
       " 1902932)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loaded = np.load(\"conversation_tokens.npy\", allow_pickle=True)\n",
    "file_loaded[1:10], len(file_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.block_size = 1024 * 8\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.input_ids) - 1) // self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):     \n",
    "        start_idx = idx * self.block_size\n",
    "        end_idx = start_idx + self.block_size\n",
    "        x = self.input_ids[start_idx:end_idx]\n",
    "        y = self.input_ids[start_idx+1:end_idx+1]\n",
    "        \n",
    "        return torch.LongTensor(x), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDataset(tokens)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open(\"chatgpt_dataset/conversations_3.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# for x, y in data[3][\"mapping\"].items():\n",
    "#     if y[\"message\"] is not None:\n",
    "#         print(y[\"message\"][\"content\"][\"parts\"])\n",
    "\n",
    "# for x, y in data[3][\"mapping\"].items():\n",
    "#     if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "#         print(data[3][\"title\"])\n",
    "#         # make first letter of role uppercase\n",
    "#         print(y[\"message\"][\"author\"][\"role\"].capitalize())\n",
    "#         print(y[\"message\"][\"content\"][\"parts\"])\n",
    "\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm  # Optional: to display progress bar\n",
    "\n",
    "# # Function to process a single file\n",
    "# def process_file(file):\n",
    "#     local_conversation = []  # Local list to store conversation for this file\n",
    "#     with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "        \n",
    "#         for item in data:\n",
    "#             chat = item[\"title\"]\n",
    "#             for x, y in item[\"mapping\"].items():\n",
    "#                 if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "#                     try:\n",
    "#                         chat += f\" {y['message']['author']['role'].capitalize()}: {y['message']['content']['parts'][0]}\"\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#             local_conversation.append(chat)\n",
    "#     return local_conversation\n",
    "\n",
    "# # Function to combine results from all files\n",
    "# def process_all_files(files):\n",
    "#     # Use a Pool of workers to process files in parallel\n",
    "#     with mp.Pool(processes=os.cpu_count()) as pool:\n",
    "#         # Using `tqdm` to track progress\n",
    "#         results = list(tqdm(pool.imap(process_file, files), total=len(files)))\n",
    "    \n",
    "#     # Combine results into a single conversation list\n",
    "#     all_conversations = []\n",
    "#     for result in results:\n",
    "#         all_conversations.extend(result)\n",
    "    \n",
    "#     return all_conversations\n",
    "\n",
    "# # List of files to process\n",
    "# files = [\"chatgpt_dataset/conversations_1.json\", \"chatgpt_dataset/conversations_2.json\", \"chatgpt_dataset/conversations_3.json\"]\n",
    "\n",
    "# # Process the files and get the conversation data\n",
    "# conversation = process_all_files(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file('config/llama_3.2_1B.gin')\n",
    "config = ModelArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1.4985B\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters in the  in billions\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params/1e9:.4f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference only decoder\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import gin\n",
    "from typing import Optional\n",
    "from functions import precompute_freqs_cis, apply_rotary_emb, repeat_kv, reshape_for_broadcast\n",
    "\n",
    "@gin.configurable\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    vocab_size: int\n",
    "    multiple_of: int\n",
    "    ffn_dim_multiplier: float\n",
    "    norm_eps: float\n",
    "    rope_theta: float\n",
    "    max_batch_size: int\n",
    "    max_seq_len: int\n",
    "    attn_dropout: float = 0.0\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initializes the RMSNorm module.\n",
    "\n",
    "        Args:\n",
    "            dim: The dimension of the input tensor.\n",
    "            eps: The epsilon value used to avoid division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Computes the RMSNorm of a tensor.\n",
    "\n",
    "        Given an input tensor `x`, compute its RMSNorm by dividing it by the root\n",
    "        mean square of its elements.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The RMSNorm of the input tensor.\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        \"\"\"\n",
    "        Computes the RMSNorm of a tensor and applies a learnable scale factor.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The RMSNorm of the input tensor multiplied by a learnable scale factor.\n",
    "        \"\"\"\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Attention module.\n",
    "\n",
    "        Args:\n",
    "            args: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of heads, and maximum sequence length.\n",
    "\n",
    "        Attributes:\n",
    "            n_heads: The number of attention heads.\n",
    "            n_kv_heads: The number of key-value heads (default: same as n_heads).\n",
    "            n_rep: The number of times to repeat key-value heads if n_kv_heads < n_heads.\n",
    "            head_dim: The dimension of each attention head.\n",
    "            wq, wk, wv, wo: Linear layers for queries, keys, values, and output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # linear layers for queries, keys, and values\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor):        \n",
    "        \"\"\"\n",
    "        Computes the output of the attention module.\n",
    "\n",
    "        Given an input tensor `x`, precomputed frequencies `freqs_cis`, and\n",
    "        configuration parameters `args`, apply the attention mechanism to produce\n",
    "        the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "            freqs_cis: The precomputed frequencies for the rotary embedding.\n",
    "\n",
    "        Returns:\n",
    "            The output of the attention module.\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # linear projections for queries, keys, and values\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # reshape for attention computation\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # apply rotary embeddings\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
    "\n",
    "        # compute attention\n",
    "        y = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True, dropout_p=self.args.attn_dropout)\n",
    "        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.n_heads * self.head_dim)\n",
    "\n",
    "        # output projection\n",
    "        return self.wo(y)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "            dim: The input dimension.\n",
    "            hidden_dim: The hidden dimension.\n",
    "            multiple_of: The multiple of the hidden dimension.\n",
    "            ffn_dim_multiplier: An optional float to multiply the hidden dimension by.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the output of the feed-forward network.\n",
    "\n",
    "        Given an input tensor `x`, apply two linear layers with the ReLU activation\n",
    "        function to produce the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after applying the feed-forward network.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerBlock module.\n",
    "\n",
    "        Args:\n",
    "            layer_id: The layer ID in the layer stack.\n",
    "            args: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of heads, and maximum sequence length.\n",
    "\n",
    "        Attributes:\n",
    "            n_heads: The number of attention heads.\n",
    "            dim: The input dimension.\n",
    "            head_dim: The dimension of each attention head.\n",
    "            attention: The attention module.\n",
    "            feed_forward: The feed-forward network module.\n",
    "            layer_id: The layer ID in the layer stack.\n",
    "            attention_norm: The normalization module for the attention module.\n",
    "            ffn_norm: The normalization module for the feed-forward network module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes the output of the transformer block.\n",
    "\n",
    "        Given an input tensor `x`, starting position `start_pos`, precomputed frequencies\n",
    "        `freqs_cis` and an optional tensor `mask`, apply the attention module and the\n",
    "        feed-forward network module to produce the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "            start_pos: The starting position of the current segment in the cache.\n",
    "            freqs_cis: The precomputed frequencies for the rotary embedding.\n",
    "            mask: An optional tensor with shape (batch_size, n_heads, seq_len, cache_len + seq_len)\n",
    "                to mask out the scores of invalid positions.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after applying the transformer block.\n",
    "        \"\"\"\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            params: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of layers, number of heads, vocabulary size, and other\n",
    "                hyperparameters.\n",
    "\n",
    "        Attributes:\n",
    "            params: Stores the configuration parameters.\n",
    "            vocab_size: The size of the vocabulary.\n",
    "            n_layers: The number of transformer layers.\n",
    "            tok_embeddings: The token embedding layer.\n",
    "            layers: A list of TransformerBlock layers.\n",
    "            norm: An RMSNorm layer for normalizing the output.\n",
    "            output: A linear layer for generating output logits.\n",
    "            freqs_cis: Precomputed frequencies for rotary embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "        # tie the weights of the token embeddings and the output layer\n",
    "        self.tok_embeddings.weight = self.output.weight\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        \"\"\"\n",
    "        Given a tensor of tokens, applies the transformer model to generate a\n",
    "        tensor of output logits.\n",
    "\n",
    "        The transformer model consists of a sequence of TransformerBlock layers.\n",
    "        Each TransformerBlock layer applies a multi-head self-attention mechanism\n",
    "        using the rotary embeddings, followed by a feed-forward network.\n",
    "\n",
    "        The method takes two arguments: `tokens` and `start_pos`. `tokens` is a\n",
    "        tensor of shape (batch_size, seq_len) containing the input tokens. `start_pos`\n",
    "        is an integer indicating the starting position of the current segment in\n",
    "        the cache.\n",
    "\n",
    "        The method first embeds the input tokens using the `tok_embeddings` layer.\n",
    "        It then applies the TransformerBlock layers sequentially to the embedded\n",
    "        tokens. The output of the last layer is normalized using the `norm` layer,\n",
    "        and the output logits are generated using the `output` layer.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, seq_len, vocab_size) containing the\n",
    "            output logits.\n",
    "        \"\"\"\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

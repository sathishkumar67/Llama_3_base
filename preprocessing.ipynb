{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2995.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm  \n",
    "\n",
    "conversation = []\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file):\n",
    "  conversation = []\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "      chat = item[\"title\"]\n",
    "      for x, y in item[\"mapping\"].items():\n",
    "        if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "          try:\n",
    "            if len(y['message']['content']['parts'][0]) > 0:\n",
    "              chat += f\" {y['message']['author']['role'].capitalize()}: {y['message']['content']['parts'][0]}\"\n",
    "          except Exception as e:\n",
    "            pass\n",
    "      conversation.append(chat)\n",
    "    return conversation\n",
    "\n",
    "# Function to combine results from all files\n",
    "def process_all_files(files):\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        results = list(tqdm(executor.map(process_file, files), total=len(files)))\n",
    "\n",
    "    # Combine results into a single conversation list\n",
    "    all_conversations = []\n",
    "    for result in results:\n",
    "        all_conversations.extend(result)\n",
    "\n",
    "    return all_conversations\n",
    "\n",
    "# Get the list of all files\n",
    "files = [os.path.join(\"chatgpt_dataset\", file) for file in os.listdir(\"chatgpt_dataset\")]\n",
    "\n",
    "# Process the files and get the conversation data\n",
    "conversation = process_all_files(files)\n",
    "\n",
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathi\\miniconda3\\envs\\dl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sathi\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1264 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "# Use ThreadPoolExecutor to tokenize texts in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Map the tokenize_text function to each string in texts\n",
    "    tokenized_texts = list(executor.map(tokenize_text, conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2395591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465322"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "tokens = list(chain.from_iterable(tokenized_texts))\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the tokenized texts to a numpy file\n",
    "file = np.array(tokens)\n",
    "np.save(\"conversation_tokens.npy\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16934, 21444,  9220, 11787,    25,  1011,   428,   309, 16934])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loaded = np.load(\"conversation_tokens.npy\", allow_pickle=True)\n",
    "file_loaded[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\sathi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conversation_tokens.npy: 100%|██████████| 9.86M/9.86M [00:04<00:00, 2.25MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/pt-sk/chatgpt-dataset/commit/36cfb10a9fbe9586d1a25bced546ed8024dac1ec', commit_message='Upload conversation_tokens.npy with huggingface_hub', commit_description='', oid='36cfb10a9fbe9586d1a25bced546ed8024dac1ec', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"pt-sk/chatgpt-dataset\"\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "login(\"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\")\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"conversation_tokens.npy\",\n",
    "    path_in_repo=\"conversation_tokens.npy\",\n",
    "    repo_id=file_name,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conversation_tokens.npy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"pt-sk/chatgpt-dataset\", filename=\"conversation_tokens.npy\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([16934, 21444,  9220, 11787,    25,  1011,   428,   309, 16934]),\n",
       " 2465322)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loaded = np.load(\"conversation_tokens.npy\", allow_pickle=True)\n",
    "file_loaded[1:10], len(file_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.block_size = 1024 * 8\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.input_ids) - 1) // self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):     \n",
    "        start_idx = idx * self.block_size\n",
    "        end_idx = start_idx + self.block_size\n",
    "        x = self.input_ids[start_idx:end_idx]\n",
    "        y = self.input_ids[start_idx+1:end_idx+1]\n",
    "        \n",
    "        return torch.LongTensor(x), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDataset(file_loaded)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gin\n",
    "from typing import Optional\n",
    "from functions import precompute_freqs_cis, apply_rotary_emb, repeat_kv\n",
    "\n",
    "# @gin.configurable\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: int = 2\n",
    "    vocab_size: int = 128256\n",
    "    multiple_of: int = 64\n",
    "    ffn_dim_multiplier: float = 1.5\n",
    "    norm_eps: float = 1e-6\n",
    "    rope_theta: float = 500000.0\n",
    "    max_batch_size: int = 1\n",
    "    max_seq_len: int = 1024 * 8\n",
    "    attn_dropout: float = 0.0\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initializes the RMSNorm module.\n",
    "\n",
    "        Args:\n",
    "            dim: The dimension of the input tensor.\n",
    "            eps: The epsilon value used to avoid division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Computes the RMSNorm of a tensor.\n",
    "\n",
    "        Given an input tensor `x`, compute its RMSNorm by dividing it by the root\n",
    "        mean square of its elements.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The RMSNorm of the input tensor.\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        \"\"\"\n",
    "        Computes the RMSNorm of a tensor and applies a learnable scale factor.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The RMSNorm of the input tensor multiplied by a learnable scale factor.\n",
    "        \"\"\"\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Attention module.\n",
    "\n",
    "        Args:\n",
    "            args: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of heads, and maximum sequence length.\n",
    "\n",
    "        Attributes:\n",
    "            n_heads: The number of attention heads.\n",
    "            n_kv_heads: The number of key-value heads (default: same as n_heads).\n",
    "            n_rep: The number of times to repeat key-value heads if n_kv_heads < n_heads.\n",
    "            head_dim: The dimension of each attention head.\n",
    "            wq, wk, wv, wo: Linear layers for queries, keys, values, and output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # linear layers for queries, keys, and values\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor):        \n",
    "        \"\"\"\n",
    "        Computes the output of the attention module.\n",
    "\n",
    "        Given an input tensor `x`, precomputed frequencies `freqs_cis`, and\n",
    "        configuration parameters `args`, apply the attention mechanism to produce\n",
    "        the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "            freqs_cis: The precomputed frequencies for the rotary embedding.\n",
    "\n",
    "        Returns:\n",
    "            The output of the attention module.\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # linear projections for queries, keys, and values\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # reshape for attention computation\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # apply rotary embeddings\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
    "\n",
    "        # compute attention\n",
    "        y = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True, dropout_p=self.args.attn_dropout)\n",
    "        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.n_heads * self.head_dim)\n",
    "\n",
    "        # output projection\n",
    "        return self.wo(y)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "            dim: The input dimension.\n",
    "            hidden_dim: The hidden dimension.\n",
    "            multiple_of: The multiple of the hidden dimension.\n",
    "            ffn_dim_multiplier: An optional float to multiply the hidden dimension by.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the output of the feed-forward network.\n",
    "\n",
    "        Given an input tensor `x`, apply two linear layers with the ReLU activation\n",
    "        function to produce the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after applying the feed-forward network.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerBlock module.\n",
    "\n",
    "        Args:\n",
    "            args: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of heads, and maximum sequence length.\n",
    "\n",
    "        Attributes:\n",
    "            n_heads: The number of attention heads.\n",
    "            dim: The input dimension.\n",
    "            head_dim: The dimension of each attention head.\n",
    "            attention: The attention module.\n",
    "            feed_forward: The feed-forward network module.\n",
    "            attention_norm: The normalization module for the attention module.\n",
    "            ffn_norm: The normalization module for the feed-forward network module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "        \"\"\"Computes the output of the transformer block.\n",
    "\n",
    "        Given an input tensor `x`, precomputed frequencies `freqs_cis`, applies the\n",
    "        attention module and the feed-forward network module to produce the output.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "            freqs_cis: The precomputed frequencies for the rotary embedding.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after applying the transformer block.\"\"\"\n",
    "        h = x + self.attention(self.attention_norm(x), freqs_cis)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            params: An instance of ModelArgs containing configuration parameters such as\n",
    "                dimensions, number of layers, number of heads, vocabulary size, and other\n",
    "                hyperparameters.\n",
    "\n",
    "        Attributes:\n",
    "            params: Stores the configuration parameters.\n",
    "            vocab_size: The size of the vocabulary.\n",
    "            n_layers: The number of transformer layers.\n",
    "            tok_embeddings: The token embedding layer.\n",
    "            layers: A list of TransformerBlock layers.\n",
    "            norm: An RMSNorm layer for normalizing the output.\n",
    "            output: A linear layer for generating output logits.\n",
    "            freqs_cis: Precomputed frequencies for rotary embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len, # here max_seq_len * 2 was used before if any error occurs change back to max_seq_len * 2\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "        # tie the weights of the token embeddings and the output layer\n",
    "        self.tok_embeddings.weight = self.output.weight\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, target: torch.Tensor=None):\n",
    "        \"\"\"Computes the output of the model.\n",
    "\n",
    "        Given an input tensor `tokens` of shape `(B, T)`, where `B` is the batch size and\n",
    "        `T` is the sequence length, applies the model to produce an output tensor of shape\n",
    "        `(B, T, V)`, where `V` is the vocabulary size.\n",
    "\n",
    "        If `target` is provided, computes the cross-entropy loss between the output and the\n",
    "        target.\n",
    "\n",
    "        Args:\n",
    "            tokens: The input tensor.\n",
    "            target: The target tensor.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two tensors, the output tensor and the loss tensor. If `target` is\n",
    "            not provided, the loss tensor is `None`.\"\"\"\n",
    "        B, T = tokens.shape\n",
    "        assert T <= self.params.max_seq_len, f\"Sequence length {T} exceeds maximum sequence length {self.params.max_seq_len}\"\n",
    "        assert B <= self.params.max_batch_size, f\"Batch size {B} exceeds maximum batch size {self.params.max_batch_size}\"\n",
    "\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, self.freqs_cis)\n",
    "        \n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ModelArgs()\n",
    "\n",
    "model = Transformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8192]), torch.Size([1, 8192]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(dataloader))\n",
    "tokens, target = item\n",
    "tokens.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8192, 128256]), tensor(11.9098, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, loss = model(tokens, target)\n",
    "output.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.761783545564427"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(1/128256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from model import ModelArgs\n",
    "\n",
    "class TokenDataset(IterableDataset):\n",
    "    def __init__(self, model_args: ModelArgs, input_file: str):\n",
    "        \"\"\"\n",
    "        Initializes the TokenDataset for lazy loading from file.\n",
    "\n",
    "        Args:\n",
    "            model_args: An instance of ModelArgs containing model configuration\n",
    "                parameters, including the maximum sequence length.\n",
    "            input_file: Path to the file containing tokenized input data.\n",
    "        \"\"\"\n",
    "        self.model_args = model_args\n",
    "        self.block_size = model_args.max_seq_len\n",
    "        self.input_file = input_file\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of blocks in the dataset.\n",
    "        \n",
    "        Since the dataset is being loaded lazily, this method could be optimized\n",
    "        or skipped for large datasets that are not fully loaded in memory.\n",
    "        \"\"\"\n",
    "        # This method is optional and can be skipped for very large datasets\n",
    "        # where length cannot be easily determined. It's provided for completeness.\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Lazily loads and yields token blocks from the input file.\n",
    "\n",
    "        Each block is a pair of x (input) and y (output) tensors of size block_size.\n",
    "\n",
    "        Yields:\n",
    "            Tuple of tensors: (input, output) tensors for the given block.\n",
    "        \"\"\"\n",
    "        # Open the file containing the tokenized data\n",
    "        with open(self.input_file, 'r') as file:\n",
    "            input_ids = []\n",
    "            for line in file:\n",
    "                # Assume each line contains a space-separated tokenized sequence\n",
    "                tokens = list(map(int, line.strip().split()))  # Convert tokens to integers\n",
    "                input_ids.extend(tokens)\n",
    "\n",
    "        # Process the tokens in blocks\n",
    "        for idx in range(0, len(input_ids) - self.block_size, self.block_size):\n",
    "            x = input_ids[idx:idx+self.block_size]\n",
    "            y = input_ids[idx+1:idx+self.block_size+1]\n",
    "\n",
    "            # Yield the current block as a tuple (x, y)\n",
    "            yield torch.LongTensor(x), torch.LongTensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open(\"chatgpt_dataset/conversations_3.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# for x, y in data[3][\"mapping\"].items():\n",
    "#     if y[\"message\"] is not None:\n",
    "#         print(y[\"message\"][\"content\"][\"parts\"])\n",
    "\n",
    "# for x, y in data[3][\"mapping\"].items():\n",
    "#     if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "#         print(data[3][\"title\"])\n",
    "#         # make first letter of role uppercase\n",
    "#         print(y[\"message\"][\"author\"][\"role\"].capitalize())\n",
    "#         print(y[\"message\"][\"content\"][\"parts\"])\n",
    "\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm  # Optional: to display progress bar\n",
    "\n",
    "# # Function to process a single file\n",
    "# def process_file(file):\n",
    "#     local_conversation = []  # Local list to store conversation for this file\n",
    "#     with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "        \n",
    "#         for item in data:\n",
    "#             chat = item[\"title\"]\n",
    "#             for x, y in item[\"mapping\"].items():\n",
    "#                 if y[\"message\"] is not None and y[\"message\"][\"author\"][\"role\"] != \"system\":\n",
    "#                     try:\n",
    "#                         chat += f\" {y['message']['author']['role'].capitalize()}: {y['message']['content']['parts'][0]}\"\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#             local_conversation.append(chat)\n",
    "#     return local_conversation\n",
    "\n",
    "# # Function to combine results from all files\n",
    "# def process_all_files(files):\n",
    "#     # Use a Pool of workers to process files in parallel\n",
    "#     with mp.Pool(processes=os.cpu_count()) as pool:\n",
    "#         # Using `tqdm` to track progress\n",
    "#         results = list(tqdm(pool.imap(process_file, files), total=len(files)))\n",
    "    \n",
    "#     # Combine results into a single conversation list\n",
    "#     all_conversations = []\n",
    "#     for result in results:\n",
    "#         all_conversations.extend(result)\n",
    "    \n",
    "#     return all_conversations\n",
    "\n",
    "# # List of files to process\n",
    "# files = [\"chatgpt_dataset/conversations_1.json\", \"chatgpt_dataset/conversations_2.json\", \"chatgpt_dataset/conversations_3.json\"]\n",
    "\n",
    "# # Process the files and get the conversation data\n",
    "# conversation = process_all_files(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
